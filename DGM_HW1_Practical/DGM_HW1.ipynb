{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BspwgQKL3Na8"
   },
   "source": [
    "<font face=\"Times New Roman\" size=5>\n",
    "<div dir=rtl align=\"center\">\n",
    "<font face=\"Times New Roman\" size=5>\n",
    "In The Name of God\n",
    "</font>\n",
    "<br>\n",
    "<img src=\"https://logoyar.com/content/wp-content/uploads/2021/04/sharif-university-logo.png\" alt=\"University Logo\" width=\"150\" height=\"150\">\n",
    "<br>\n",
    "<font face=\"Times New Roman\" size=4 align=center>\n",
    "Sharif University of Technology - Department of Electrical Engineering\n",
    "</font>\n",
    "<br>\n",
    "<font color=\"#008080\" size=6>\n",
    "Deep Generative Models\n",
    "</font>\n",
    "<hr/>\n",
    "<font color=\"#800080\" size=5>\n",
    "Assignment 1 : Deep Autoregressive Models\n",
    "<br>\n",
    "</font>\n",
    "<font size=5>\n",
    "Instructor: Dr. S. Amini\n",
    "<br>\n",
    "</font>\n",
    "<font size=4>\n",
    "Fall 2024\n",
    "<br>\n",
    "</font>\n",
    "<font face=\"Times New Roman\" size=4>\n",
    "Deadline: Month day at 23:55\n",
    "</font>\n",
    "<hr>\n",
    "<font color='red'  size=4>\n",
    "Note: It is highly recommended to run your notebook on Google Colab or Kaggle\n",
    "<br>\n",
    "</font>\n",
    "<font face=\"Times New Roman\" size=4 align=center>\n",
    "Feel free to ask your questions in Telegram : @ilia_ad7\n",
    "</font>\n",
    "<br>\n",
    "<hr>\n",
    "</div></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ut_yixNLmF8Y"
   },
   "outputs": [],
   "source": [
    "Name = \"\"\n",
    "StudentId = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Deep autoregressive models are sequence models, yet feed-forward (i.e. not recurrent); generative models, yet supervised. They are a compelling alternative to RNNs for sequential data, and GANs for generation tasks.\n",
    "\n",
    "Moreover, a deep autoregressive model is merely a feed-forward model which predicts future values from past values.\n",
    "\n",
    "However, you may think that the term **Autoregressive** is equivalent to **Deep Autoregressive**; But actually, it is not true! Autoregressive Linear Models like `ARMA` or `ARCH` have been used in statistics, econometrics and financial modelling for ages and are not **Deep**. In this exercise, we work with deep ones. \n",
    "\n",
    "### Mathematical Description\n",
    "The mathematical foundation of autoregressive models is based on the chain rule of probability. We can factorize the joint distribution over the n-dimensions as:\n",
    "\n",
    "$$p(x) = \\prod_{i=1}^{n} p(x_i | x_1, x_2, …, x_{i-1}) = \\prod_{i=1}^{n} p(x_i | x_{<i})$$\n",
    "\n",
    "where $x_{<i} = [x_1, x_2, …, x_{i-1}]$ denotes the vector of random variables with index less than $i$.\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "Autoregressive models are naturally strong generative models that constitute one of the current state-of-the-art architectures on likelihood-based image modeling, and are also the basis for large language generation models such as GPT3. Similar to the language generation, autoregressive models work on images by modeling the likelihood of a pixel given all previous ones. For instance, in the picture below, we model the pixel $x_i$ as a conditional probability distribution based on all previous (here blue) pixels (figure credit - [Aaron van den Oord et al.](https://arxiv.org/abs/1601.06759)):\n",
    "\n",
    "<center style=\"padding: 10px\"><img src=\"AR_ImageProcessing.svg\" height=\"300px\"></center>\n",
    "\n",
    "Learning these conditionals is often much simpler than learning the joint distribution $p(\\mathbf{x})$ all together. However, disadvantages of autoregressive models include slow sampling, especially for large images, as we need height-times-width forward passes through the model. In addition, for some applications, we require a latent space as modeled in VAEs and Normalizing Flows. For instance, in autoregressive models, we cannot interpolate between two images because of the lack of a latent representation. \n",
    "\n",
    "### Some Architectures and Applications of Autoregressive Models\n",
    "\n",
    "- `PixelCNN` by Google DeepMind was probably the first deep autoregressive model, and the progenitor of most of the other models below. Ironically, the authors spend the bulk of the paper discussing a recurrent model, PixelRNN, and consider PixelCNN as a “workaround” to avoid excessive computation. However, PixelCNN is probably this paper’s most lasting contribution.\n",
    "- `PixelCNN++` by OpenAI is, unsurprisingly, PixelCNN but with various improvements.\n",
    "- `WaveNet` by Google DeepMind is heavily inspired by PixelCNN, and models raw audio, not just encoded music. They had to pull a neat trick from telecommunications/signals processing in order to cope with the sheer size of audio (high-quality audio involves at least 16-bit precision samples, which means a 65,536-way-softmax per time step!)\n",
    "- `Transformer`, a.k.a. the “attention is all you need” model by Google Brain is now a mainstay of NLP, performing very well at many NLP tasks and being incorporated into subsequent models like BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Pre-requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q yfinance\n",
    "!pip install --quiet pytorch-lightning>=1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYZ8RhPxk6ed"
   },
   "source": [
    "# Question 1 : LSTM\n",
    "\n",
    "### **The Model**\n",
    "In this exercise, we will explore the application of Long Short-Term Memory (LSTM) networks, a type of recurrent neural network, to predict stock prices. LSTMs are particularly suited for this task due to their ability to capture long-term dependencies in time series data.\n",
    "\n",
    "We will treat this as an autoregressive problem, where we use past stock prices to predict future ones. Autoregressive models are widely used in time series forecasting, and they can capture complex patterns in the data.\n",
    "\n",
    "The goal of this exercise is to familiarize you with the process of training an LSTM network and using it for prediction. You will learn how to preprocess time series data, visualize it, construct an LSTM model, train it on historical stock price data, and evaluate its performance.\n",
    "\n",
    "Remember, predicting stock prices is a complex task and the aim of this exercise is not to create a model for making actual investment decisions, but rather to understand the potential applications of LSTM networks in time series forecasting.\n",
    "\n",
    "### **The Dataset**\n",
    "The initial phase of our process involves the acquisition and loading of requisite data into memory. Our primary data source will be Yahoo Finance, a comprehensive platform renowned for its extensive repository of financial market data and its suite of tools designed to identify promising investment opportunities. To facilitate the extraction of data from Yahoo Finance, we will employ the yfinance library, which provides a Pythonic and multi-threaded approach to downloading market data.\n",
    "\n",
    "## Load the Data\n",
    "\n",
    "The process begins with data acquisition from Yahoo Finance and loading it into memory. Yahoo Finance is utilized for its extensive financial market data. The yfinance library, known for its efficient and Python-friendly approach, is used to download this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lE0dgerIk14v",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries here. You may add other libraries to suit your needs.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import pandas_datareader as pdr\n",
    "from pandas_datareader.data import DataReader\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "set_seed(47)\n",
    "\n",
    "# Set up plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "%matplotlib inline\n",
    "\n",
    "yf\n",
    "\n",
    "# Define the list of tech stocks for analysis\n",
    "tech_list = ['AAPL', 'GOOG', 'MSFT', 'AMZN']\n",
    "\n",
    "# Set up the start and end times for data grab\n",
    "end = datetime.now()\n",
    "start = datetime(end.year - 1, end.month, end.day)\n",
    "\n",
    "# Download stock data and assign it to global variables\n",
    "for stock in tech_list:\n",
    "    globals()[stock] = yf.download(stock, start, end)\n",
    "\n",
    "# Define the list of companies and their names\n",
    "company_list = [AAPL, GOOG, MSFT, AMZN]\n",
    "company_name = [\"APPLE\", \"GOOGLE\", \"MICROSOFT\", \"AMAZON\"]\n",
    "\n",
    "# Add company name to each dataframe\n",
    "for company, com_name in zip(company_list, company_name):\n",
    "    company[\"company_name\"] = com_name\n",
    "\n",
    "# Concatenate all company dataframes\n",
    "df = pd.concat(company_list, axis=0)\n",
    "\n",
    "# Display the last 10 rows of the dataframe\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis\n",
    "\n",
    "The `.describe()` function generates descriptive statistics that summarize the central tendency, dispersion, and shape of a dataset's distribution, excluding `NaN` values. It can analyze both numeric and object series, as well as DataFrame column sets of mixed data types. The output varies based on the input data.\n",
    "\n",
    "Use this function to generate a concise statistical summary of one of the stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##### Step 1: Statistical summary #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the `.info()` method, provide a concise summary of the DataFrame. This includes information such as:\n",
    "1. The number of entries (rows)\n",
    "2. The number of columns\n",
    "3. The names of the columns\n",
    "4. The number of non-null values in each column\n",
    "5. The data type of each column (e.g., integer, float, object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##### Step 2: DataFrame summary #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The closing price, the last traded price during a regular trading day, is a key benchmark for tracking stock performance.\n",
    "Let's visualize the historical closing prices for our list of companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##### Step 3: Visualize the historical closing prices #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volume refers to the number of shares traded during a certain period, often a day. It's a key metric for technical traders.\n",
    "Let's visualize the daily trading volume for our list of companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##### Step 4: Visualize the daily trading volume #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stock moving average (MA) is a widely used technical indicator in financial analysis that helps smooth out price trends by filtering out the noise from random short-term price fluctuations. It is calculated by taking the arithmetic mean of a given set of prices over a specified period.\n",
    "\n",
    "The moving average can be of different types, such as Simple Moving Average (SMA) and Exponential Moving Average (EMA). SMA is calculated by taking the arithmetic mean of a given set of prices over a specified period. On the other hand, EMA places greater weight on more recent prices than older ones over the time period.\n",
    "\n",
    "Here, you should calculate and plot the 10, 20, and 50-day moving averages along with the adjusted close price for each company. The moving averages are calculated using the `rolling` method, which applies a function (in this case, mean) to a rolling window of data. The plots provide a visual representation of the stock price trends over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define moving average days\n",
    "ma_day = [10, 20, 50]\n",
    "\n",
    "##### Step 5: Calculate and plot the 10, 20, and 50-day moving averages along with the adjusted close price #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now answer this **Question:**\n",
    "\n",
    "Which moving average is the best choice for predicting the trends? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain your **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've done some baseline analysis, let's go ahead and dive a little deeper. We're now going to analyze the risk of the stock. In order to do so we'll need to take a closer look at the daily changes of the stock, and not just its absolute value. Let's go ahead and use pandas to retrieve daily returns for all stocks. We'll use `pct_change` to find the percent change for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##### Step 6: Plot the daily return precentages #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get an overall look at the daily returns by creating a histogram for the `Daily Return` of each company in the `company_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##### Step 7: Plot the histogram of Daily Returns #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between Different Stocks Closing Prices\n",
    "\n",
    "Correlation is a statistical measure that quantifies the degree of relationship between two variables. It provides an indication of how changes in one variable are associated with changes in another. The correlation coefficient ranges between -1.0 and +1.0. A correlation of +1.0 indicates a perfect positive correlation, meaning that both variables move in the same direction together. A correlation of -1.0 indicates a perfect negative correlation, meaning that the variables move in opposite directions. If the correlation is 0, it means there is no relationship between the variables. However, it's important to note that correlation does not imply causation. Even if two variables are correlated, it does not mean that changes in one variable are causing changes in another. There could be a third factor influencing both variables.\n",
    "\n",
    "In the context of stock analysis, suppose we are interested in examining the returns of all the stocks in our list. To facilitate this, we can construct a DataFrame that consolidates the `Adj Close` columns from each of the individual stock dataframes. This will provide us with a comprehensive view of the closing prices for all stocks, enabling more efficient comparative analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get the closing prices for all stocks in the tech list\n",
    "data = yf.download(tech_list, start=start, end=end)\n",
    "\n",
    "# Select only the 'Adj Close' column for each stock\n",
    "closing_df = data['Adj Close']\n",
    "\n",
    "# Calculate the daily returns for each stock\n",
    "tech_rets = closing_df.pct_change()\n",
    "\n",
    "# Display the first few rows of the returns DataFrame\n",
    "tech_rets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the daily percentage return of all pairs to check how they are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##### Step 8: Plot the correlation regression plot for all pairs #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, answer this **Question:**\n",
    "\n",
    "What implications might a linear relationship have for investment strategies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain your **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, Let's create two heatmaps: one for the correlation between the daily returns of the stocks and another for the correlation between the closing prices of the stocks. This provides a visual and numerical understanding of the relationships between different stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##### Step 9: Plot correlation heatmaps #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, answer this **Question:** \n",
    "\n",
    "Which pair has the strongest correlation of daily return stocks? What other interesting facts could be infered from the plots above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain your **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Closing Price\n",
    "\n",
    "In this section, we will try to predict the closing price of a stock using an LSTM model. Choose one stock and after visualizing the closing history, do any preprocessing you need and create a training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##### Step 10: Download and display the head of a stock from Yahoo Finance (start='2012-01-01') #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##### Step 11: Plot the closing price history of your stock #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##### Step 12: Preparing the data (Use a 95-5 Train/Test ratio) #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##### Step 13: Scaling the data #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##### Step 14: Creating the training and test data sets #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define your model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##### Step 15: Define your model ######\n",
    "# LSTM Model\n",
    "# Define LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        pass\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "        return out\n",
    "\n",
    "# Set Hyper-parameters\n",
    "num_epochs = ...\n",
    "learning_rate = ...\n",
    "input_size = ...\n",
    "hidden_size1 = ...\n",
    "hidden_size2 = ...\n",
    "num_layers = ...\n",
    "seq_length = ...\n",
    "output_size = ...\n",
    "\n",
    "lstm = LSTM(input_size, hidden_size1, hidden_size2, num_layers, output_size)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##### Step 16: Train the model #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##### Step 17: Predict price values & Calcualte RMSE for Test set ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##### Step 18: Plot the entire closing price history & Compare it to your predictions ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Show the valid and predicted prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional\n",
    "You have the opportunity to develop an autoregressive model based on the concepts you learned in class and subsequently train it. Following that, you can compare its performance with that of the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Bonus ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 : PixelCNN\n",
    "\n",
    "In this question, we implement an autoregressive likelihood model for the task of image modeling. Our implementation will focus on the PixelCNN [2] model. Most current SOTA models use PixelCNN as their fundamental architecture, and various additions have been proposed to improve the performance (e.g. PixelCNN++ and PixelSNAIL).\n",
    "\n",
    "\n",
    "## Import Libraries and Requirements\n",
    "First of all, we need to import our standard libraries. We will use PyTorch Lightning here for the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import math\n",
    "import numpy as np \n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import seaborn as sns\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional\n",
    "import torch.utils.data as data\n",
    "import torch.optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
    "DATASET_PATH = \"../data\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Fetching the device that will be used throughout this notebook\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is for the further usages. Just run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs('../saved_model', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "We will work on the MNIST dataset and use 8-bits per pixel (values between 0 and 255). The dataset is loaded below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert images from 0-1 to 0-255 (integers). We use the long datatype as we will use the images as labels as well\n",
    "def discretize(sample):\n",
    "    return (sample * 255).to(torch.long)\n",
    "\n",
    "# Transformations applied on each image => only make them a tensor\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                discretize])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_dataset = MNIST(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "pl.seed_everything(42)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [50000, 10000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = MNIST(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "val_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good practice is to always visualize some data examples to get an intuition of the data. You should complete the following function to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def show_imgs(imgs):\n",
    "    \"\"\"\n",
    "    Function to display images in a grid.\n",
    "    TODO: Implement the function that displays 'imgs' in a grid.\n",
    "    \n",
    "    Args:\n",
    "    - imgs: a batch of images in tensor format or a list of images\n",
    "    \n",
    "    You can use matplotlib or other libraries to visualize the images.\n",
    "    \"\"\"\n",
    "    pass \n",
    "\n",
    "show_imgs([train_set[i][0] for i in range(8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Autoregressive Convolutions\n",
    "\n",
    "The core module of PixelCNN is its masked convolutions. In contrast to language models, we don't apply an LSTM on each pixel one-by-one. This would be inefficient because images are grids instead of sequences. Thus, it is better to rely on convolutions that have shown great success in deep CNN classification models.\n",
    "\n",
    "Nevertheless, we cannot just apply standard convolutions without any changes. Remember that during training of autoregressive models, we want to use teacher forcing which both helps the model training, and significantly reduces the time needed for training. For image modeling, teacher forcing is implemented by using a training image as input to the model, and we want to obtain as output the prediction for each pixel based on only its predecessors. Thus, we need to ensure that the prediction for a specific pixel can only be influenced by its predecessors and not by its own value or any \"future\" pixels. For this, we apply convolutions with a mask.\n",
    "\n",
    "Which mask we use depends on the ordering of pixels we decide on, i.e. which is the first pixel we predict, which is the second one, etc. The most commonly used ordering is to denote the upper left pixel as the start pixel, and sort the pixels row by row, as shown in the visualization at the top of the assignment. Thus, the second pixel is on the right of the first one (first row, second column), and once we reach the end of the row, we start in the second row, first column. If we now want to apply this to our convolutions, we need to ensure that the prediction of pixel 1 is not influenced by its own \"true\" input, and all pixels on its right and in any lower row. In convolutions, this means that we want to set those entries of the weight matrix to zero that take pixels on the right and below into account. As an example for a $5\\times 5$ kernel, see a mask below (figure credit - Aaron van den Oord):\n",
    "\n",
    "<center style=\\\"padding: 10px\\\"><img src=\"Masked_Conv.svg\" width=\"200px\"></center>\n",
    "\n",
    "Before looking into the application of masked convolutions in PixelCNN in detail, let's first implement a module that allows us to apply an arbitrary mask to a convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MaskedConvolution(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_out, mask, **kwargs):\n",
    "        \"\"\"\n",
    "        Implements a convolution with mask applied on its weights.\n",
    "        Inputs:\n",
    "            c_in - Number of input channels\n",
    "            c_out - Number of output channels\n",
    "            mask - Tensor of shape [kernel_size_H, kernel_size_W] with 0s where\n",
    "                   the convolution should be masked, and 1s otherwise.\n",
    "            kwargs - Additional arguments for the convolution\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # For simplicity: calculate padding automatically \n",
    "        kernel_size = (mask.shape[0], mask.shape[1])\n",
    "        dilation = 1 if \"dilation\" not in kwargs else kwargs[\"dilation\"]\n",
    "        padding = tuple([dilation*(kernel_size[i]-1)//2 for i in range(2)])\n",
    "        # Actual convolution\n",
    "        self.conv = nn.Conv2d(c_in, c_out, kernel_size, padding=padding, **kwargs)\n",
    "        \n",
    "        # Mask as buffer => it is no parameter but still a tensor of the module \n",
    "        # (must be moved with the devices)\n",
    "        self.register_buffer('mask', mask[None,None])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.conv.weight.data *= self.mask # Ensures zero's at masked positions\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertical and horizontal convolution stacks\n",
    "\n",
    "To build our own autoregressive image model, we could simply stack a few masked convolutions on top of each other. This was actually the case for the original PixelCNN model, but this leads to a considerable issue. When sequentially applying a couple of masked convolutions, the receptive field of a pixel show to have a \"blind spot\" on the right upper side, as shown in the figure below (figure credit - Aaron van den Oord et al.):\n",
    "\n",
    "<center style=\"padding: 10px\"><img src=\"PCNN_BlindSpot.svg\" height=\"300px\"></center>\n",
    "\n",
    "Although a pixel should be able to take into account all other pixels above and left of it, a stack of masked convolutions does not allow us to look to the upper pixels on the right. This is because the features of the pixels above, which we use for convolution, do not contain any information of the pixels on the right of the same row. If they would, we would be \"cheating\" and actually looking into the future. To overcome this issue, van den Oord et. al proposed to split the convolutions into a vertical and a horizontal stack. The vertical stack looks at all pixels above the current one, while the horizontal takes into account all on the left. While keeping both of them separate, we can actually look at the pixels on the right with the vertical stack without breaking any of our assumptions. The two convolutions are also shown in the figure above.\n",
    "\n",
    "Let us implement them here as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VerticalStackConvolution(MaskedConvolution):\n",
    "    \n",
    "    def __init__(self, c_in, c_out, kernel_size=3, mask_center=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a vertical stack masked convolution where pixels below are masked out.\n",
    "        Inputs:\n",
    "            c_in - Number of input channels\n",
    "            c_out - Number of output channels\n",
    "            kernel_size - Size of the convolution kernel\n",
    "            mask_center - If True, mask out the center pixel as well\n",
    "        \"\"\"\n",
    "        # TODO: Create the mask for vertical stack convolution\n",
    "        \n",
    "        # TODO: Mask out all pixels below the current row\n",
    "        \n",
    "        # TODO: Mask out the center row if mask_center is True\n",
    "        \n",
    "        super().__init__(c_in, c_out, mask, **kwargs)\n",
    "\n",
    "\n",
    "class HorizontalStackConvolution(MaskedConvolution):\n",
    "    \n",
    "    def __init__(self, c_in, c_out, kernel_size=3, mask_center=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a horizontal stack masked convolution where pixels to the right are masked out.\n",
    "        Inputs:\n",
    "            c_in - Number of input channels\n",
    "            c_out - Number of output channels\n",
    "            kernel_size - Size of the convolution kernel\n",
    "            mask_center - If True, mask out the center pixel as well\n",
    "        \"\"\"\n",
    "        # TODO: Create the mask for horizontal stack convolution\n",
    "        \n",
    "        # TODO: Mask out all pixels to the right of the current pixel\n",
    "        \n",
    "        # TODO: Mask out the center pixel if mask_center is True\n",
    "        \n",
    "        super().__init__(c_in, c_out, mask, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have an input argument called `mask_center`. Remember that the input to the model is the actual input image. Hence, the very first convolution we apply cannot use the center pixel as input, but must be masked. All consecutive convolutions, however, should use the center pixel as we otherwise lose the features of the previous layer. Hence, the input argument `mask_center` is True for the very first convolutions, and False for all others.\n",
    "\n",
    "## Visualizing the receptive field \n",
    "\n",
    "To validate our implementation of masked convolutions, we can visualize the receptive field we obtain with such convolutions. We should see that with increasing number of convolutional layers, the receptive field grows in both vertical and horizontal direction, without the issue of a blind spot. The receptive field can be empirically measured by backpropagating an arbitrary loss for the output features of a speicifc pixel with respect to the input. We implement this idea below, and visualize the receptive field below.\n",
    "\n",
    "<font color='red'> DO NOT edit cells of this section </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inp_img = torch.zeros(1, 1, 11, 11)\n",
    "inp_img.requires_grad_()\n",
    "\n",
    "def show_center_recep_field(img, out):\n",
    "    \"\"\"\n",
    "    Calculates the gradients of the input with respect to the output center pixel,\n",
    "    and visualizes the overall receptive field.\n",
    "    Inputs:\n",
    "        img - Input image for which we want to calculate the receptive field on.\n",
    "        out - Output features/loss which is used for backpropagation, and should be\n",
    "              the output of the network/computation graph.\n",
    "    \"\"\"\n",
    "    # Determine gradients\n",
    "    loss = out[0,:,img.shape[2]//2,img.shape[3]//2].sum() # L1 loss for simplicity\n",
    "    loss.backward(retain_graph=True) # Retain graph as we want to stack multiple layers and show the receptive field of all of them\n",
    "    img_grads = img.grad.abs()\n",
    "    img.grad.fill_(0) # Reset grads\n",
    "    \n",
    "    # Plot receptive field\n",
    "    img = img_grads.squeeze().cpu().numpy()\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    pos = ax[0].imshow(img)\n",
    "    ax[1].imshow(img>0)\n",
    "    # Mark the center pixel in red if it doesn't have any gradients (should be the case for standard autoregressive models)\n",
    "    show_center = (img[img.shape[0]//2,img.shape[1]//2] == 0)\n",
    "    if show_center:\n",
    "        center_pixel = np.zeros(img.shape + (4,))\n",
    "        center_pixel[center_pixel.shape[0]//2,center_pixel.shape[1]//2,:] = np.array([1.0, 0.0, 0.0, 1.0]) \n",
    "    for i in range(2):\n",
    "        ax[i].axis('off')\n",
    "        if show_center:\n",
    "            ax[i].imshow(center_pixel)\n",
    "    ax[0].set_title(\"Weighted receptive field\")\n",
    "    ax[1].set_title(\"Binary receptive field\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "show_center_recep_field(inp_img, inp_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first visualize the receptive field of a horizontal convolution without the center pixel. We use a small, arbitrary input image (pixels), and calculate the loss for the center pixel. For simplicity, we initialize all weights with 1 and the bias with 0, and use a single channel. This is sufficient for our visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "horiz_conv = HorizontalStackConvolution(c_in=1, c_out=1, kernel_size=3, mask_center=True)\n",
    "horiz_conv.conv.weight.data.fill_(1)\n",
    "horiz_conv.conv.bias.data.fill_(0)\n",
    "horiz_img = horiz_conv(inp_img)\n",
    "show_center_recep_field(inp_img, horiz_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The receptive field is shown in yellow, the center pixel in red, and all other pixels outside of the receptive field are dark blue. As expected, the receptive field of a single horizontal convolution with the center pixel masked and a \n",
    " kernel is only the pixel on the left. If we use a larger kernel size, more pixels would be taken into account on the left.\n",
    "\n",
    "Next, let's take a look at the vertical convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vert_conv = VerticalStackConvolution(c_in=1, c_out=1, kernel_size=3, mask_center=True)\n",
    "vert_conv.conv.weight.data.fill_(1)\n",
    "vert_conv.conv.bias.data.fill_(0)\n",
    "vert_img = vert_conv(inp_img)\n",
    "show_center_recep_field(inp_img, vert_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vertical convolution takes all pixels above into account. Combining these two, we get the $L$-shaped receptive field of the original masked convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "horiz_img = vert_img + horiz_img\n",
    "show_center_recep_field(inp_img, horiz_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we stack multiple horizontal and vertical convolutions, we need to take two aspects into account:\n",
    "\n",
    "The center should not be masked anymore for the following convolutions as the features at the pixel's position are already independent of its actual value. If it is hard to imagine why we can do this, just change the value below to `mask_center=True` and see what happens.\n",
    "The vertical convolution is not allowed to work on features from the horizontal convolution. In the feature map of the horizontal convolutions, a pixel contains information about all of the \"true\" pixels on the left. If we apply a vertical convolution which also uses features from the right, we effectively expand our receptive field to the true input which we want to prevent. Thus, the feature maps can only be merged for the horizontal convolution.\n",
    "Using this, we can stack the convolutions in the following way. We have two feature streams: one for the vertical stack, and one for the horizontal stack. The horizontal convolutions can operate on the joint features of the previous horizontals and vertical convolutions, while the vertical stack only takes its own previous features as input. For a quick implementation, we can therefore sum the horizontal and vertical output features at each layer, and use those as final output features to calculate the loss on. An implementation of 4 consecutive layers is shown below. Note that we reuse the features from the other convolutions with `mask_center=True` from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize convolutions with equal weight to all input pixels\n",
    "horiz_conv = HorizontalStackConvolution(c_in=1, c_out=1, kernel_size=3, mask_center=False)\n",
    "horiz_conv.conv.weight.data.fill_(1)\n",
    "horiz_conv.conv.bias.data.fill_(0)\n",
    "vert_conv = VerticalStackConvolution(c_in=1, c_out=1, kernel_size=3,mask_center=False)\n",
    "vert_conv.conv.weight.data.fill_(1)\n",
    "vert_conv.conv.bias.data.fill_(0)\n",
    "\n",
    "# We reuse our convolutions for the 4 layers here. Note that in a standard network,\n",
    "# we don't do that, and instead learn 4 separate convolution. As this cell is only for\n",
    "# visualization purposes, we reuse the convolutions for all layers.\n",
    "for l_idx in range(4):\n",
    "    vert_img = vert_conv(vert_img)\n",
    "    horiz_img = horiz_conv(horiz_img) + vert_img\n",
    "    print(f\"Layer {l_idx+2}\")\n",
    "    show_center_recep_field(inp_img, horiz_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The receptive field above it visualized for the horizontal stack, which includes the features of the vertical convolutions. It grows over layers without any blind spot as we had before. The difference between \"weighted\" and \"binary\" receptive field is that for the latter, we check whether there are any gradients flowing back to this pixel. This indicates that the center pixel indeed can use information from this pixel. Nevertheless, due to the convolution weights, some pixels have a stronger effect on the prediction than others. This is visualized in the weighted receptive field by plotting the gradient magnitude for each pixel instead of a binary yes/no.\n",
    "\n",
    "Another receptive field we can check is the one for the vertical stack as the one above is for the horizontal stack. Let's visualize it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "show_center_recep_field(inp_img, vert_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have discussed before, the vertical stack only looks at pixels above the one we want to predict. Hence, we can validate that our implementation works as we initially expected it to. As a final step, let's clean up the computation graph we still had kept in memory for the visualization of the receptive field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "del inp_img, horiz_conv, vert_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Convolutions\n",
    "\n",
    "For visualizing the receptive field, we assumed a very simplified stack of vertical and horizontal convolutions. Obviously, there are more sophisticated ways of doing it, and PixelCNN uses gated convolutions for this. Specifically, the Gated Convolution block in PixelCNN looks as follows:\n",
    "\n",
    "<center width=\"100%\" style=\"padding: 10px\"><img src=\"PCNN_GatedConv.svg\" height=\"300px\"></center>\n",
    "\n",
    "The left path is the vertical stack (the $N\\times N$ convolution is masked correspondingly), and the right path is the horizontal stack. Gated convolutions are implemented by having a twice as large output channel size, and combine them by a element-wise multiplication of $\\tanh$ and a sigmoid. For a linear layer, we can express a gated activation unit as follows:\n",
    "\n",
    "$$\\mathbf{y} = \\tanh\\left(\\mathbf{W}_{f}\\mathbf{x}\\right)\\odot\\sigma\\left(\\mathbf{W}_{g}\\mathbf{x}\\right)$$\n",
    "\n",
    "For simplicity, biases have been neglected and the linear layer split into two part, $\\mathbf{W}_{f}$ and $\\mathbf{W}_{g}$. This concept resembles the input and modulation gate in an LSTM, and has been used in many other architectures as well. The main motivation behind this gated activation is that it might allow to model more complex interactions and simplifies learning. But as in any other architecture, this is mostly a design choice and can be considered a hyperparameters.\n",
    "\n",
    "Besides the gated convolutions, we also see that the horizontal stack uses a residual connection while the vertical stack does not. This is because we use the output of the horizontal stack for prediction. Each convolution in the vertical stack also receives a strong gradient signal as it is only two $1\\times 1$ convolutions away from the residual connection, and does not require another residual connection to all its earleri layers.\n",
    "\n",
    "The implementation in PyTorch is fairly straight forward for this block, because the visualization above gives us a computation graph to follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GatedMaskedConv(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, **kwargs):\n",
    "        \"\"\"\n",
    "        Gated Convolution block implementing the computation graph.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize vertical and horizontal stack convolutions\n",
    "        self.conv_vert = VerticalStackConvolution(c_in, c_out=2*c_in, **kwargs)\n",
    "        self.conv_horiz = HorizontalStackConvolution(c_in, c_out=2*c_in, **kwargs)\n",
    "        \n",
    "        # TODO: Add a 1x1 convolution to pass information from vertical stack to horizontal stack\n",
    "        # Hint: Use nn.Conv2d with 2*c_in channels for both input and output\n",
    "        \n",
    "        # TODO: Add a 1x1 convolution at the output of the horizontal stack\n",
    "        # Hint: Use nn.Conv2d with c_in channels for both input and output\n",
    "    \n",
    "    def forward(self, v_stack, h_stack):\n",
    "        # Vertical stack (left)\n",
    "        v_stack_feat = self.conv_vert(v_stack)\n",
    "        \n",
    "        # TODO: Split the vertical stack features into value and gate using chunk\n",
    "        \n",
    "        # TODO: Apply gated activation to the value and gate\n",
    "        # Hint: Use torch.tanh for the value and torch.sigmoid for the gate\n",
    "        v_stack_out = ...\n",
    "        \n",
    "        # Horizontal stack (right)\n",
    "        h_stack_feat = self.conv_horiz(h_stack)\n",
    "        \n",
    "        # TODO: Add the transformed vertical stack features to the horizontal stack\n",
    "        \n",
    "        # TODO: Split the horizontal stack features into value and gate using chunk\n",
    "        \n",
    "        # TODO: Apply gated activation to the value and gate\n",
    "        \n",
    "        # TODO: Apply the 1x1 convolution to the horizontal stack output\n",
    "        h_stack_out = ...\n",
    "        \n",
    "        # TODO: Add residual connection to the horizontal stack output\n",
    "        \n",
    "        return v_stack_out, h_stack_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Using the gated convolutions, we can now build our PixelCNN model. The architecture consists of multiple stacked GatedMaskedConv blocks, where we add an additional dilation factor to a few convolutions. This is used to increase the receptive field of the model and allows to take a larger context into accout during generation. Dilation on a convolution works looks as follows:\n",
    "\n",
    "<center width=\\\"100%\\\"><img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/dilation.gif\" width=\"250px\"></center>\n",
    "\n",
    "Note that the smaller output size is only because the animation assumes no padding. In our implementation, we will pad the input image correspondingly. Alternatively to dilated convolutions, we could downsample the input and use a encoder-decoder architecture as in PixelCNN++ [3]. This is especially beneficial if we want to build a very deep autoregressive model. Nonetheless, as we seek to train a reasonably small model, dilated convolutions are the more efficient option to use here.\n",
    "\n",
    "Below, we implement the PixelCNN model as a PyTorch Lightning module. Besides the stack of gated convolutions, we also have the initial horizontal and vertical convolutions which mask the center pixel, and a final $1\\times 1$ convolution which maps the output features to class predictions. To determine the likelihood of a batch of images, we first create our initial features using the masked horizontal and vertical input convolution. Next, we forward the features through the stack of gated convolutions. Finally, we take the output features of the horizontal stack, and apply the $1\\times 1$ convolution for classification. We use the bits per dimension metric for the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PixelCNN(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, c_in, c_hidden):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Initial convolutions skipping the center pixel\n",
    "        # TODO: Initialize vertical stack convolution\n",
    "        # TODO: Initialize horizontal stack convolution\n",
    "        \n",
    "        # TODO: Create a list of GatedMaskedConv layers with different dilations.\n",
    "        # Hint: Use nn.ModuleList to store the layers\n",
    "        \n",
    "        # Output classification convolution (1x1)\n",
    "        self.conv_out = nn.Conv2d(c_hidden, c_in * 256, kernel_size=1, padding=0) \n",
    "        \n",
    "        self.example_input_array = train_set[0][0][None]  # Optional, for showing an example input in PyTorch Lightning\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward image through model and return logits for each pixel.\n",
    "        Inputs:\n",
    "            x - Image tensor with integer values between 0 and 255.\n",
    "        \"\"\"\n",
    "        # TODO: Scale input from 0-255 to -1 to 1\n",
    "        \n",
    "        # TODO: Apply the initial vertical and horizontal stack convolutions to x\n",
    "        \n",
    "        # Gated Convolutions\n",
    "        for layer in self.conv_layers:\n",
    "            # TODO: Pass the vertical and horizontal stacks through each layer\n",
    "            pass\n",
    "        \n",
    "        # TODO: Apply ELU activation and 1x1 convolution to the horizontal stack output\n",
    "        out = ...\n",
    "        \n",
    "        # TODO: Reshape the output to match the pixel classification problem, Output dimensions: [Batch, Classes, Channels, Height, Width]\n",
    "        out = ...\n",
    "        return out\n",
    "    \n",
    "    def calc_likelihood(self, x):\n",
    "        # TODO: Implement the forward pass and calculate the negative log-likelihood\n",
    "        # Hint: Use F.cross_entropy to compute the loss and return bpd\n",
    "        pass\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def sample(self, img_shape, img=None):\n",
    "        \"\"\"\n",
    "        Sampling function for the autoregressive model.\n",
    "        Inputs:\n",
    "            img_shape - Shape of the image to generate (B,C,H,W)\n",
    "            img (optional) - If given, this tensor will be used as\n",
    "                             a starting image. The pixels to fill \n",
    "                             should be -1 in the input tensor.\n",
    "        \"\"\"\n",
    "        # TODO: Initialize the image with zeros if img is None\n",
    "        \n",
    "        # Generation loop\n",
    "        for h in tqdm(range(img_shape[2]), leave=False):\n",
    "            for w in range(img_shape[3]):\n",
    "                for c in range(img_shape[1]):\n",
    "                    # TODO: Skip already filled pixels\n",
    "                    \n",
    "                    # TODO: Predict pixel values and sample from the output distribution\n",
    "                    \n",
    "        return img\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # TODO: Create an Adam optimizer and StepLR scheduler\n",
    "        optimizer = ...\n",
    "        scheduler = ...\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # TODO: Calculate the loss and log it during training\n",
    "        loss = ...\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # TODO: Calculate the loss and log it during validation\n",
    "        pass\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # TODO: Calculate the loss and log it during testing\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample from the autoregressive model, we need to iterate over all dimensions of the input. We start with an empty image, and fill the pixels one by one, starting from the upper left corner. Note that as for predicting $x_i$, all pixels below it have no influence on the prediction. Hence, we can cut the image in height without changing the prediction while increasing efficiency. Nevertheless, all the loops in the sampling function already show that it will take us quite some time to sample. A lot of computation could be reused across loop iterations as those the features on the already predicted pixels will not change over iterations. Nevertheless, this takes quite some effort to implement, and is often not done in implementations because in the end, autoregressive sampling remains sequential and slow. Hence, we settle with the default implementation here.\n",
    "\n",
    "Before training the model, we can check the full receptive field of the model on an MNIST image of size $28\\times 28$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_model = PixelCNN(c_in=1, c_hidden=64)\n",
    "inp = torch.zeros(1,1,28,28)\n",
    "inp.requires_grad_()\n",
    "out = test_model(inp)\n",
    "show_center_recep_field(inp, out.squeeze(dim=2))\n",
    "del inp, out, test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization shows that for predicting any pixel, we can take almost half of the image into account. However, keep in mind that this is the \"theoretical\" receptive field and not necessarily the effective receptive field, which is usually much smaller. For a stronger model, we should therefore try to increase the receptive field even further. Especially, for the pixel on the bottom right, the very last pixel, we would be allowed to take into account the whole image. However, our current receptive field only spans across 1/4 of the image. An encoder-decoder architecture can help with this, but it also shows that we require a much deeper, more complex network in autoregressive models than in VAEs or energy-based models.\n",
    "\n",
    "## Training loop\n",
    "To train the model, we again can rely on PyTorch Lightning and write a function below for loading the pretrained model if it exists. **In this section, you may not be able to train the model more than 20 or 30 epochs, as you are using Google Colab or kaggle probably. Consequently, we have saved the pre-trained (with 150 epochs) model for you to use it and write and validate other details of your code (e.g. visualizations, sampling, autocompletions, ...) except building the model and training procedure. Then, just derive your own model (by completing corresponding preious and further sections) and run it to reach some acceptable results. The saved model in \"PixelCNN.ckpt\"** Moreover, To reduce the computational cost, we have saved the validation and test score in the checkpoint already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(**kwargs):\n",
    "    # TODO: Create a PyTorch Lightning trainer with the necessary settings\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join('../saved_model', \"PixelCNN\"), \n",
    "                         ..., \n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"val_bpd\"),\n",
    "                                    LearningRateMonitor(\"epoch\")])\n",
    "    # Hint: Set the correct device (GPU/CPU) and define callbacks like ModelCheckpoint and LearningRateMonitor\n",
    "    \n",
    "    result = None\n",
    "    \n",
    "    # Path to the pre-trained checkpoint (for example on Kaggle)\n",
    "    pretrained_filename = '/kaggle/input/pixelcnn-ckpt/PixelCNN.ckpt'  # TODO: Set the correct path for the pre-trained model\n",
    "    \n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        # TODO: Load the pre-trained model from checkpoint using PixelCNN\n",
    "        pass\n",
    "    else:\n",
    "        # Initialize PixelCNN with the provided arguments (kwargs)\n",
    "        model = PixelCNN(**kwargs)\n",
    "        # TODO: Train the model using trainer.fit and provide the training and validation loaders\n",
    "        pass\n",
    "    \n",
    "    # TODO: Move the model to the correct device (CPU or GPU)\n",
    "    \n",
    "    if result is None:\n",
    "        # TODO: Test the best model on the validation and test sets using trainer.test\n",
    "        # Hint: Use the validation and test loaders, and store the results in 'result'\n",
    "        pass\n",
    "    \n",
    "    return model, result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model, result = train_model(c_in=1, c_hidden=64)\n",
    "test_res = result[\"test\"][0]\n",
    "print(\"Test bits per dimension: %4.3fbpd\" % (test_res[\"test_loss\"] if \"test_loss\" in test_res else test_res[\"test_bpd\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, answer the **Question:**\n",
    "\n",
    "Why considering image modeling as an autoregressive problem simplifies the learning process?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain your **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, PixelCNN can explicitly predict the pixel values by a discrete softmax while Normalizing Flows (which you will learn further) have to learn transformations in continuous latent space. These two aspects allow the PixelCNN to achieve a notably better performance.\n",
    "\n",
    "To fully compare the models, let's also measure the number of parameters of the PixelCNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_params = sum([np.prod(param.shape) for param in model.parameters()])\n",
    "print(\"Number of parameters: {:,}\".format(num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the multi-scale normalizing flows, the PixelCNN has considerably less parameters. Of course, the number of parameters depend on our hyperparameter choices. Nevertheless, in general, it can be said that autoregressive models require considerably less parameters than normalizing flows to reach good performance, based on the reasons stated above. Still, autoregressive models are much slower in sampling than normalizing flows, which limits their possible applications.\n",
    "\n",
    "## Sampling\n",
    "One way of qualitatively analysing generative models is by looking at the actual samples. Let's therefore use our sampling function to generate a few digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO: Ensure the model is moved to the correct device (GPU or CPU)\n",
    "\n",
    "# TODO: Set a random seed using pl.seed_everything() for reproducibility\n",
    "\n",
    "# TODO: Generate samples using the model's sample function with the correct image shape\n",
    "# Hint: Make sure to move the generated samples to the same device as the model\n",
    "\n",
    "# TODO: Display the generated samples using show_imgs()\n",
    "# Hint: Move samples back to the CPU if needed for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the samples should be identified as digits. This goes along with the lower likelihood we achieved with autoregressive models. Nevertheless, we also see that there is still place for improvement as a considerable amount of samples cannot be identified. Deeper autoregressive models are expected to achieve better quality, as they can take more context into account for generating the pixels.\n",
    "\n",
    "Note that on Google Colab or Kaggle, you might see different results, specifically with a white line at the top. After some debugging, it seemed that the difference occurs inside the dilated convolution, as it gives different results for different batch sizes. However, it is hard to debug this further as it might be a bug of the installed PyTorch version on them.\n",
    "\n",
    "The trained model itself is not restricted to any specific image size. However, what happens if we actually sample a larger image than we had seen in our training dataset? Let's try below to sample images of size $64\\times 64$ instead of $28\\times 28$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO: Fill it like previous cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, answer the **Question:**\n",
    "\n",
    "What do you see in the results? Why do you think it is like that? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain your **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocompletion\n",
    "One common application done with autoregressive models is auto-completing an image. As autoregressive models predict pixels one by one, we can set the first $N$ pixels to predefined values and check how the model completes the image. For implementing this, we just need to skip the iterations in the sampling loop that already have a value unequals -1. In the cell below, we randomly take three images from the training set, mask about the lower half of the image, and let the model autocomplete it. To see the diversity of samples, we do this 12 times for each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocomplete_image(img):\n",
    "    # TODO: Create a copy of the image (img.clone()) and set the lower half of the image to -1\n",
    "\n",
    "    print(\"Original image and input image to sampling:\")\n",
    "    # TODO: Display the original and modified input images using show_imgs\n",
    "\n",
    "    # TODO: Generate 12 copies of the incomplete image for sampling using img_init.unsqueeze and repeat\n",
    "    # Hint: Ensure the images are moved to the correct device\n",
    "    \n",
    "    # TODO: Set a random seed for reproducibility before sampling\n",
    "\n",
    "    # TODO: Generate image completions using the model's sample function\n",
    "    img_generated = ...\n",
    "\n",
    "    print(\"Autocompletion samples:\")\n",
    "    show_imgs(img_generated)\n",
    "\n",
    "# Autocomplete images from the dataset (train_set)\n",
    "for i in range(1, 4):\n",
    "    img = train_set[i][0]\n",
    "    autocomplete_image(img.to(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, answer the **Question:**\n",
    "\n",
    "What do you see in the results? Why do you think it is like that? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain your **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] van den Oord, A., et al. \"Pixel Recurrent Neural Networks.\" arXiv preprint arXiv:1601.06759 (2016).\n",
    "\n",
    "[2] van den Oord, A., et al. \"Conditional Image Generation with PixelCNN Decoders.\" In Advances in Neural Information Processing Systems 29, pp. 4790–4798 (2016).\n",
    "\n",
    "[3] Salimans, Tim, et al. \"PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications.\" arXiv preprint arXiv:1701.05517 (2017)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5772424,
     "sourceId": 9488246,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
